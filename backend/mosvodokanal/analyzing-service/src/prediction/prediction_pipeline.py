from typing import List, Dict, Any
from datetime import datetime, timedelta
import pickle
import json
from models.messages import ITPDataMessage
import statsmodels

#####

json_input_str = '''
[
    {
        "itp": {
            "id": "4fd787ff-8d98-4fee-bfe2-42749f425817",
            "number": "–ò–¢–ü-7090-33"
        },
        "mkd": {
            "address": "–ë–µ–≥–æ–≤–∞—è —É–ª., 32",
            "fias": "b2f1a5bd-e41e-44df-8d8f-275dbffed5da",
            "unom": "47571371",
            "latitude": null,
            "longitude": null,
            "district": null
        },
        "odpuGvsDevices": [
            {
                "heatMeterIdentifier": "f462f8c1-76d1-42e8-9d28-8109e059405f",
                "firstChannelFlowmeterIdentifier": "4c346e4b-68fe-45fb-848e-bd93db2f5eb4",
                "secondChannelFlowmeterIdentifier": "f5650ddb-3905-494d-bc58-086664cb314c",
                "firstChannelFlowValue": 7.084,
                "secondChannelFlowValue": 9.685
            }
        ],
        "waterMeters": [
            {
                "identifier": "3bfd83f5-92fc-4efc-b02f-3df143a8960f",
                "flowValue": 8.267
            }
        ],
        "timestamp": 1759356889132,
        "itpid": "4fd787ff-8d98-4fee-bfe2-42749f425817"
    },
    {
        "itp": {
            "id": "4fd787ff-8d98-4fee-bfe2-42749f425817",
            "number": "–ò–¢–ü-7090-33"
        },
        "mkd": {
            "address": "–ë–µ–≥–æ–≤–∞—è —É–ª., 32",
            "fias": "b2f1a5bd-e41e-44df-8d8f-275dbffed5da",
            "unom": "47571371",
            "latitude": null,
            "longitude": null,
            "district": null
        },
        "odpuGvsDevices": [
            {
                "heatMeterIdentifier": "f462f8c1-76d1-42e8-9d28-8109e059405f",
                "firstChannelFlowmeterIdentifier": "4c346e4b-68fe-45fb-848e-bd93db2f5eb4",
                "secondChannelFlowmeterIdentifier": "f5650ddb-3905-494d-bc58-086664cb314c",
                "firstChannelFlowValue": 7.084,
                "secondChannelFlowValue": 9.685
            }
        ],
        "waterMeters": [
            {
                "identifier": "3bfd83f5-92fc-4efc-b02f-3df143a8960f",
                "flowValue": 8.267
            }
        ],
        "timestamp": 1759366899132,
        "itpid": "4fd787ff-8d98-4fee-bfe2-42749f425817"
    },
    {
        "itp": {
            "id": "4fd787ff-8d98-4fee-bfe2-42749f425817",
            "number": "–ò–¢–ü-7090-33"
        },
        "mkd": {
            "address": "–ë–µ–≥–æ–≤–∞—è —É–ª., 32",
            "fias": "b2f1a5bd-e41e-44df-8d8f-275dbffed5da",
            "unom": "47571371",
            "latitude": null,
            "longitude": null,
            "district": null
        },
        "odpuGvsDevices": [
            {
                "heatMeterIdentifier": "f462f8c1-76d1-42e8-9d28-8109e059405f",
                "firstChannelFlowmeterIdentifier": "4c346e4b-68fe-45fb-848e-bd93db2f5eb4",
                "secondChannelFlowmeterIdentifier": "f5650ddb-3905-494d-bc58-086664cb314c",
                "firstChannelFlowValue": 7.084,
                "secondChannelFlowValue": 9.685
            }
        ],
        "waterMeters": [
            {
                "identifier": "3bfd83f5-92fc-4efc-b02f-3df143a8960f",
                "flowValue": 8.267
            }
        ],
        "timestamp": 1759376899132,
        "itpid": "4fd787ff-8d98-4fee-bfe2-42749f425817"
    },
    {
        "itp": {
            "id": "4fd787ff-8d98-4fee-bfe2-42749f425817",
            "number": "–ò–¢–ü-7090-33"
        },
        "mkd": {
            "address": "–ë–µ–≥–æ–≤–∞—è —É–ª., 32",
            "fias": "b2f1a5bd-e41e-44df-8d8f-275dbffed5da",
            "unom": "47571371",
            "latitude": null,
            "longitude": null,
            "district": null
        },
        "odpuGvsDevices": [
            {
                "heatMeterIdentifier": "f462f8c1-76d1-42e8-9d28-8109e059405f",
                "firstChannelFlowmeterIdentifier": "4c346e4b-68fe-45fb-848e-bd93db2f5eb4",
                "secondChannelFlowmeterIdentifier": "f5650ddb-3905-494d-bc58-086664cb314c",
                "firstChannelFlowValue": 7.084,
                "secondChannelFlowValue": 9.685
            }
        ],
        "waterMeters": [
            {
                "identifier": "3bfd83f5-92fc-4efc-b02f-3df143a8960f",
                "flowValue": 8.267
            }
        ],
        "timestamp": 1759386899132,
        "itpid": "4fd787ff-8d98-4fee-bfe2-42749f425817"
    }
]
'''

#####

class PredictionPipeline:
    """–ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞–Ω–∏–π –¥–∞—Ç—á–∏–∫–æ–≤"""
    
    def __init__(self):
        self.sarimax_models = {}  # –∫–ª—é—á: —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö, –∑–Ω–∞—á–µ–Ω–∏–µ: –º–æ–¥–µ–ª—å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    
    def _determine_model_type(self, series_name: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –º–æ–¥–µ–ª–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Å–µ—Ä–∏–∏ –∏–∑ training_data_info
        
        Args:
            series_name: –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–µ—Ä–∏–∏ –¥–∞–Ω–Ω—ã—Ö (–∏–∑ training_data_info)
            
        Returns:
            –¢–∏–ø –º–æ–¥–µ–ª–∏: 'supply_gvs', 'return_gvs', 'consumption_xvs'
        """
        series_name_lower = series_name.lower()
        
        if '–ø–æ–¥–∞—á–∞' in series_name_lower or 'supply' in series_name_lower:
            return 'supply_gvs'
        elif '–æ–±—Ä–∞—Ç–∫–∞' in series_name_lower or 'return' in series_name_lower:
            return 'return_gvs'
        elif '–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ' in series_name_lower or 'consumption' in series_name_lower or '—Ö–≤—Å' in series_name_lower:
            return 'consumption_xvs'
        else:
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –º–æ–¥–µ–ª–∏ –¥–ª—è series_name: {series_name}")
    
    def load_sarimax_models(self, model_paths: Dict[str, str]):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ SARIMAX –º–æ–¥–µ–ª–µ–π –∏–∑ pickle —Ñ–∞–π–ª–æ–≤
        
        Args:
            model_paths: —Å–ª–æ–≤–∞—Ä—å —Å –ø—É—Ç—è–º–∏ –∫ –º–æ–¥–µ–ª—è–º
                –∫–ª—é—á–∏: –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤
                –∑–Ω–∞—á–µ–Ω–∏—è: –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º .pkl
        """
        for file_name, path in model_paths.items():
            try:
                with open(path, 'rb') as f:
                    loaded_package = pickle.load(f)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ –ø–æ series_name –∏–∑ training_data_info
                series_name = loaded_package['training_data_info']['series_name']
                model_type = self._determine_model_type(series_name)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ç–∏–ø–æ–º
                self.sarimax_models[model_type] = {
                    'model': loaded_package['model'],
                    'order': loaded_package['order'],
                    'seasonal_order': loaded_package['seasonal_order'],
                    'training_info': loaded_package['training_data_info'],
                    'fit_date': loaded_package['fit_date'],
                    'source_file': file_name
                }
                
                print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_type} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {file_name}")
                print(f"   Series: {series_name}")
                print(f"   Order: {loaded_package['order']}")
                print(f"   Seasonal Order: {loaded_package['seasonal_order']}")
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏–∑ {file_name}: {e}")
    
    def get_loaded_models_info(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö"""
        info = {}
        for model_type, model_data in self.sarimax_models.items():
            info[model_type] = {
                'series_name': model_data['training_info']['series_name'],
                'order': model_data['order'],
                'seasonal_order': model_data['seasonal_order'],
                'fit_date': model_data['fit_date'],
                'data_length': model_data['training_info'].get('data_length', 'unknown')
            }
        return info
    
    def extract_data_from_messages(self, messages: List[ITPDataMessage]) -> List[Dict[str, Any]]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ ITPDataMessage –≤ –ø—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–∞—Ç
        
        Args:            messages: —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –æ—Ç –¥–∞—Ç—á–∏–∫–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        extracted_data = []
        
        for message in messages:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ì–í–° (–±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –ø—Ä–∏–±–æ—Ä –µ—Å–ª–∏ –µ—Å—Ç—å)
            supply_gvs = None
            return_gvs = None
            temp1_gvs = None
            temp2_gvs = None
            
            if message.odpu_gvs_devices:
                device = message.odpu_gvs_devices[0]
                supply_gvs = device.first_channel_flow_value
                return_gvs = device.second_channel_flow_value
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –•–í–° (–±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —Å—á–µ—Ç—á–∏–∫ –µ—Å–ª–∏ –µ—Å—Ç—å)
            consumption_xvs = None
            if message.water_meters:
                meter = message.water_meters[0]
                consumption_xvs = meter.flow_value
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö
            data_record = {
                'timestamp': message.timestamp,
                'supply_gvs': supply_gvs,
                'return_gvs': return_gvs,
                'consumption_xvs': consumption_xvs,
                'temp1_gvs': temp1_gvs,
                'temp2_gvs': temp2_gvs,
                'itp_id': str(message.itp.id),
                'itp_number': message.itp.number
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ
            if any(v is not None for v in [supply_gvs, return_gvs, consumption_xvs]):
                extracted_data.append(data_record)
        
        return extracted_data
    
    def _retrain_model_with_new_data(self, model_type: str, new_valid_data: List[Dict[str, Any]]) -> bool:
        """
        –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            model_type: —Ç–∏–ø –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è
            new_valid_data: –Ω–æ–≤—ã–µ –≤–∞–ª–∏–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            
        Returns:
            True –µ—Å–ª–∏ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—à–ª–æ —É—Å–ø–µ—à–Ω–æ, False –≤ –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ
        """
        try:
            if model_type not in self.sarimax_models:
                print(f"‚ùå –ú–æ–¥–µ–ª—å {model_type} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è")
                return False
            
            model_info = self.sarimax_models[model_type]
            old_model = model_info['model']
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö
            historical_values = []
            timestamps = []
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞
            print(new_valid_data)
            for data_point in new_valid_data:
                value = data_point.get(model_type)
                if value is not None:
                    historical_values.append(value)
                    timestamps.append(data_point['timestamp'])
            
            if not historical_values:
                print(f"‚ùå –ù–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è {model_type}")
                return False
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            sorted_data = sorted(zip(timestamps, historical_values), key=lambda x: x[0])
            sorted_values = [value for _, value in sorted_data]
            
            print(f"üîÑ –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {model_type} –Ω–∞ {len(sorted_values)} —Ç–æ—á–∫–∞—Ö –¥–∞–Ω–Ω—ã—Ö...")
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å —Å —Ç–µ–º–∏ –∂–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            from statsmodels.tsa.statespace.sarimax import SARIMAX
            
            new_model = SARIMAX(
                sorted_values,
                order=model_info['order'],
                seasonal_order=model_info['seasonal_order'],
                enforce_stationarity=False,
                enforce_invertibility=False
            )
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ç–∞—Ä–æ–π –º–æ–¥–µ–ª–∏ –∫–∞–∫ –Ω–∞—á–∞–ª—å–Ω—ã–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
            start_params = old_model.params
            
            # –û–±—É—á–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
            new_result = new_model.fit(
                start_params=start_params,
                disp=False,
                method='lbfgs',
                maxiter=100
            )
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–æ–¥–µ–ª—å –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            model_info['model'] = new_result
            model_info['fit_date'] = datetime.now()
            model_info['training_info']['data_length'] = len(sorted_values)
            model_info['training_info']['last_date'] = timestamps[-1] if timestamps else None
            
            print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_type} —É—Å–ø–µ—à–Ω–æ –¥–æ–æ–±—É—á–µ–Ω–∞")
            print(f"   –ù–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(sorted_values)}")
            print(f"   –î–∞—Ç–∞ –¥–æ–æ–±—É—á–µ–Ω–∏—è: {datetime.now()}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ {model_type}: {e}")
            return False
    
    def process_batch(self, messages: List[ITPDataMessage], forecast_hours: int = 24, retrain_models: bool = True) -> Dict[str, Any]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
        
        Args:
            messages: —Å–ø–∏—Å–æ–∫ –≤—Ö–æ–¥—è—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
            forecast_hours: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
            retrain_models: –Ω—É–∂–Ω–æ –ª–∏ –¥–æ–æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∞–º–∏
        """
        results = {
            'processed_messages': len(messages),
            'anomalies_detected': 0,
            'valid_data_count': 0,
            'models_retrained': [],
            'forecasts': {},
            'anomaly_details': [],
            'processing_time': datetime.now()
        }
        
        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏–π
        current_data_list = self.extract_data_from_messages(messages)
        
        if not current_data_list:
            print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return results
        
        print(f"üìä –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(current_data_list)} –∑–∞–ø–∏—Å–µ–π –¥–∞–Ω–Ω—ã—Ö")
        
        if retrain_models and self.sarimax_models:
            print("üîÑ –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
            for model_type in self.sarimax_models.keys():
                if self._retrain_model_with_new_data(model_type, current_data_list):
                    results['models_retrained'].append(model_type)
        
        # 5. –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å –º–æ–¥–µ–ª–∏
        if self.sarimax_models:
            forecasts = self._generate_forecasts(forecast_hours)
            results['forecasts'] = forecasts
            print(f"üìà –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤: {len(forecasts)}")
        else:
            print("‚ö†Ô∏è –ú–æ–¥–µ–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã, –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ")
        
        return results
    
    def _generate_forecasts(self, hours: int, from_date: datetime = None) -> Dict[str, List[Dict[str, Any]]]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º SARIMAX –º–æ–¥–µ–ª–µ–π
        
        Args:
            hours: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø—Ä–æ–≥–Ω–æ–∑–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö
        """
        forecasts = {}
        
        for model_type, model_info in self.sarimax_models.items():
            try:
                model = model_info['model']
                
                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑
                forecast_values = model.forecast(steps=hours)
                
                # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
                last_timestamp = from_date if from_date else datetime.now()
                forecast_dates = [last_timestamp + timedelta(hours=i+1) for i in range(hours)]
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ —Ç–æ—á–µ–∫
                forecast_points = []
                for i, value in enumerate(forecast_values):
                    point = {
                        'timestamp': forecast_dates[i],
                        'value': float(value),
                        'type': model_type,
                        'series_name': model_info['training_info']['series_name']
                    }
                    forecast_points.append(point)
                
                forecasts[model_type] = forecast_points
                
                print(f"‚úÖ –ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è {model_type}: {len(forecast_points)} —Ç–æ—á–µ–∫")
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è {model_type}: {e}")
                forecasts[model_type] = []
        
        return forecasts
    
    def save_models(self, output_dir: str = "./models"):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        
        Args:
            output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        """
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        for model_type, model_info in self.sarimax_models.items():
            try:
                model_package = {
                    'model': model_info['model'],
                    'model_type': 'SARIMAX',
                    'order': model_info['order'],
                    'seasonal_order': model_info['seasonal_order'],
                    'training_data_info': model_info['training_info'],
                    'fit_date': model_info['fit_date']
                }
                
                filename = f"sarimax_model_{model_type}_{datetime.now().strftime('%Y%m%d_%H%M')}.pkl"
                filepath = os.path.join(output_dir, filename)
                
                with open(filepath, 'wb') as f:
                    pickle.dump(model_package, f)
                
                print(f"üíæ –ú–æ–¥–µ–ª—å {model_type} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {filepath}")
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ {model_type}: {e}")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    
    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞
    pipeline = PredictionPipeline()
    
    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    historical_data = [
        {
            'timestamp': datetime(2025, 4, 26, 10, 0, 0),
            'supply_gvs': 0.15,
            'return_gvs': 0.10,
            'consumption_xvs': 0.07,
            'temp1_gvs': 52.0,
            'temp2_gvs': 31.0
        },
        # ... –±–æ–ª—å—à–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    ]
    
    # 3. –ó–∞–≥—Ä—É–∑–∫–∞ SARIMAX –º–æ–¥–µ–ª–µ–π
    model_paths = {
        'model_gvs_podacha': 'models/sarimax_model_gvs_podacha.pkl',
        'model_gvs_obratka': 'models/sarimax_model_gvs_obratka.pkl', 
        'model_xvs': 'models/sarimax_model_xvs.pkl'
    }
    
    pipeline.load_sarimax_models(model_paths)
    
    # 4. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö
    models_info = pipeline.get_loaded_models_info()
    print("\nüìã –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
    for model_type, info in models_info.items():
        print(f"   {model_type}: {info['series_name']}")
    
    # 5. –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥—è—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —Å –¥–æ–æ–±—É—á–µ–Ω–∏–µ–º
    sample_messages = [

    ]  # List[ITPDataMessage]

    data_list = json.loads(json_input_str)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç —Å–ª–æ–≤–∞—Ä—è –≤ –æ–±—ä–µ–∫—Ç ITPDataMessage
    sample_messages = [ITPDataMessage.from_dict(item) for item in data_list]
    
    results = pipeline.process_batch(
        sample_messages, 
        forecast_hours=24, 
        retrain_models=True  # –í–∫–ª—é—á–µ–Ω–æ –¥–æ–æ–±—É—á–µ–Ω–∏–µ
    )
    # 6. –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
    print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {results['processed_messages']}")
    print(f"   –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {results['anomalies_detected']}")
    print(f"   –í–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {results['valid_data_count']}")
    print(f"   –î–æ–æ–±—É—á–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(results['models_retrained'])}")
    print(f"   –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤: {len(results['forecasts'])}")
    
    # 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    if results['models_retrained']:
        pipeline.save_models()
        print(f"üíæ –î–æ–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")


if __name__ == "__main__":
    main()